{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:52:13.203643Z","iopub.status.busy":"2024-10-28T04:52:13.203145Z","iopub.status.idle":"2024-10-28T04:52:31.710069Z","shell.execute_reply":"2024-10-28T04:52:31.709092Z","shell.execute_reply.started":"2024-10-28T04:52:13.203591Z"},"trusted":true},"outputs":[],"source":["from transformers import AutoImageProcessor, AutoModelForImageClassification\n","import torch.nn.functional as F\n","import numpy as np\n","import PIL\n","import torch\n","from pathlib import Path\n","import pandas as pd\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision.transforms import Compose, Normalize, Resize, CenterCrop, ToTensor\n","import gc\n","from numba import cuda\n","\n","id2label = {\n","    0: \"Cassava Bacterial Blight (CBB)\",\n","    1: \"Cassava Brown Streak Disease (CBSD)\",\n","    2: \"Cassava Green Mottle (CGM)\",\n","    3: \"Cassava Mosaic Disease (CMD)\",\n","    4: \"Healthy\",\n","}\n","label2id = {\n","    \"Cassava Bacterial Blight (CBB)\": 0,\n","    \"Cassava Brown Streak Disease (CBSD)\": 1,\n","    \"Cassava Green Mottle (CGM)\": 2,\n","    \"Cassava Mosaic Disease (CMD)\": 3,\n","    \"Healthy\": 4,\n","}\n","\n","folder = Path(\"/kaggle/input/cassava-leaf-disease-classification/test_images\")"]},{"cell_type":"markdown","metadata":{},"source":["# ViT full"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:52:37.706188Z","iopub.status.busy":"2024-10-28T04:52:37.705009Z","iopub.status.idle":"2024-10-28T04:52:41.727981Z","shell.execute_reply":"2024-10-28T04:52:41.727123Z","shell.execute_reply.started":"2024-10-28T04:52:37.706140Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/input/sc4000-vit-large/models\"\n","\n","model = AutoModelForImageClassification.from_pretrained(\n","    model_path,\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True,\n",")\n","image_processor = AutoImageProcessor.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:52:46.104812Z","iopub.status.busy":"2024-10-28T04:52:46.104019Z","iopub.status.idle":"2024-10-28T04:52:46.113067Z","shell.execute_reply":"2024-10-28T04:52:46.112011Z","shell.execute_reply.started":"2024-10-28T04:52:46.104746Z"},"trusted":true},"outputs":[],"source":["class CassavaDatasetViT(Dataset):\n","    def __init__(self, folder, image_processor):\n","        self.folder = folder\n","        self.image_processor = image_processor\n","        self.image_paths = list(folder.glob(\"*\"))\n","        self.image_mean, self.image_std = (\n","            self.image_processor.image_mean,\n","            self.image_processor.image_std,\n","        )\n","        size = self.image_processor.size[\"height\"]\n","        normalize = Normalize(mean=self.image_mean, std=self.image_std)\n","        self.test_transforms = Compose(\n","            [\n","                Resize(size),\n","                CenterCrop(size),\n","                ToTensor(),\n","                normalize,\n","            ]\n","        )\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        with PIL.Image.open(image_path) as image:\n","            inputs = self.test_transforms(image.convert(\"RGB\"))\n","        return inputs, image_path.name"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:52:50.626175Z","iopub.status.busy":"2024-10-28T04:52:50.625785Z","iopub.status.idle":"2024-10-28T04:53:01.026210Z","shell.execute_reply":"2024-10-28T04:53:01.025168Z","shell.execute_reply.started":"2024-10-28T04:52:50.626139Z"},"trusted":true},"outputs":[],"source":["vit_outputs = []\n","\n","dataset = CassavaDatasetViT(folder, image_processor)\n","dataloader = DataLoader(dataset, batch_size=16)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","with torch.no_grad():\n","    for batch, image_names in dataloader:\n","        outputs = model(batch.to(device))\n","        probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()\n","\n","        vit_outputs.extend(\n","            {\"image_id\": image_name, \"output\": output}\n","            for image_name, output in zip(image_names, probabilities)\n","        )"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:53:20.662310Z","iopub.status.busy":"2024-10-28T04:53:20.661485Z","iopub.status.idle":"2024-10-28T04:53:20.666569Z","shell.execute_reply":"2024-10-28T04:53:20.665624Z","shell.execute_reply.started":"2024-10-28T04:53:20.662267Z"},"trusted":true},"outputs":[],"source":["vit_outputs = {x[\"image_id\"]: x[\"output\"] for x in vit_outputs}"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:53:26.041069Z","iopub.status.busy":"2024-10-28T04:53:26.040355Z","iopub.status.idle":"2024-10-28T04:53:27.432255Z","shell.execute_reply":"2024-10-28T04:53:27.431467Z","shell.execute_reply.started":"2024-10-28T04:53:26.041014Z"},"trusted":true},"outputs":[],"source":["model.cpu()\n","del model\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# ConvNeXt-V2 Base"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:53:45.127890Z","iopub.status.busy":"2024-10-28T04:53:45.126964Z","iopub.status.idle":"2024-10-28T04:53:46.477240Z","shell.execute_reply":"2024-10-28T04:53:46.476253Z","shell.execute_reply.started":"2024-10-28T04:53:45.127846Z"},"trusted":true},"outputs":[],"source":["model_path = \"/kaggle/input/sc4000-convnext-v2-base/models\"\n","model = AutoModelForImageClassification.from_pretrained(\n","    model_path,\n","    id2label=id2label,\n","    label2id=label2id,\n","    ignore_mismatched_sizes=True,\n",")\n","image_processor = AutoImageProcessor.from_pretrained(model_path)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:53:48.732176Z","iopub.status.busy":"2024-10-28T04:53:48.731285Z","iopub.status.idle":"2024-10-28T04:53:48.740245Z","shell.execute_reply":"2024-10-28T04:53:48.739326Z","shell.execute_reply.started":"2024-10-28T04:53:48.732128Z"},"trusted":true},"outputs":[],"source":["class CassavaDatasetConvNeXtV2(Dataset):\n","    def __init__(self, folder, image_processor):\n","        self.folder = folder\n","        self.image_processor = image_processor\n","        self.image_paths = list(folder.glob(\"*\"))\n","        self.image_mean, self.image_std = (\n","            self.image_processor.image_mean,\n","            self.image_processor.image_std,\n","        )\n","        size = self.image_processor.size[\"shortest_edge\"]\n","        normalize = Normalize(mean=self.image_mean, std=self.image_std)\n","        self.test_transforms = Compose(\n","            [\n","                Resize(size),\n","                CenterCrop(size),\n","                ToTensor(),\n","                normalize,\n","            ]\n","        )\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        image_path = self.image_paths[idx]\n","        with PIL.Image.open(image_path) as image:\n","            inputs = self.test_transforms(image.convert(\"RGB\"))\n","        return inputs, image_path.name"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:53:55.260721Z","iopub.status.busy":"2024-10-28T04:53:55.259997Z","iopub.status.idle":"2024-10-28T04:53:58.138811Z","shell.execute_reply":"2024-10-28T04:53:58.138009Z","shell.execute_reply.started":"2024-10-28T04:53:55.260681Z"},"trusted":true},"outputs":[],"source":["convnext_outputs = []\n","\n","dataset = CassavaDatasetConvNeXtV2(folder, image_processor)\n","dataloader = DataLoader(dataset, batch_size=16)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","model.eval()\n","\n","with torch.no_grad():\n","    for batch, image_names in dataloader:\n","        outputs = model(batch.to(device))\n","        # predictions = outputs.logits.argmax(dim=-1).cpu().numpy()\n","        probabilities = F.softmax(outputs.logits, dim=-1).cpu().numpy()\n","\n","        convnext_outputs.extend(\n","            {\"image_id\": image_name, \"output\": output}\n","            for image_name, output in zip(image_names, probabilities)\n","        )"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:06.321971Z","iopub.status.busy":"2024-10-28T04:54:06.321585Z","iopub.status.idle":"2024-10-28T04:54:06.326729Z","shell.execute_reply":"2024-10-28T04:54:06.325722Z","shell.execute_reply.started":"2024-10-28T04:54:06.321933Z"},"trusted":true},"outputs":[],"source":["convnext_outputs = {x[\"image_id\"]: x[\"output\"] for x in convnext_outputs}"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:09.042842Z","iopub.status.busy":"2024-10-28T04:54:09.042077Z","iopub.status.idle":"2024-10-28T04:54:09.511728Z","shell.execute_reply":"2024-10-28T04:54:09.510928Z","shell.execute_reply.started":"2024-10-28T04:54:09.042801Z"},"trusted":true},"outputs":[],"source":["model.cpu()\n","del model\n","gc.collect()\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{},"source":["# CropNet (MobileNetV3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:23.576866Z","iopub.status.busy":"2024-10-28T04:54:23.576505Z","iopub.status.idle":"2024-10-28T04:54:29.303669Z","shell.execute_reply":"2024-10-28T04:54:29.302801Z","shell.execute_reply.started":"2024-10-28T04:54:23.576832Z"},"trusted":true},"outputs":[],"source":["from huggingface_hub import from_pretrained_keras\n","import tf_keras as keras\n","from pathlib import Path\n","import tensorflow as tf\n","from PIL import Image\n","\n","model = from_pretrained_keras(\"/kaggle/input/cropnet-mobilenetv3/models\")\n","\n","image_size = 224\n","resize_scale = 1.5\n","image_resize_shape = int(resize_scale * image_size)\n","batch_size = 32"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:40.591469Z","iopub.status.busy":"2024-10-28T04:54:40.591075Z","iopub.status.idle":"2024-10-28T04:54:40.598432Z","shell.execute_reply":"2024-10-28T04:54:40.597409Z","shell.execute_reply.started":"2024-10-28T04:54:40.591431Z"},"trusted":true},"outputs":[],"source":["val_transforms = [\n","    lambda img: tf.image.resize(\n","        img, (image_resize_shape, image_resize_shape)\n","    ),\n","    lambda img: tf.image.resize_with_crop_or_pad(\n","        img, target_height=image_size, target_width=image_size\n","    ),\n","    lambda img: img / 255.0,\n","]\n","\n","def val_image_transforms(image):\n","    for fn in val_transforms:\n","        image = fn(image)\n","    return image\n","\n","def open_image(path):\n","    with Image.open(path) as image:\n","        image = keras.utils.img_to_array(image)\n","    return val_image_transforms(image)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:45.861282Z","iopub.status.busy":"2024-10-28T04:54:45.860901Z","iopub.status.idle":"2024-10-28T04:54:45.929133Z","shell.execute_reply":"2024-10-28T04:54:45.928308Z","shell.execute_reply.started":"2024-10-28T04:54:45.861247Z"},"trusted":true},"outputs":[],"source":["folder = Path(\"/kaggle/input/cassava-leaf-disease-classification/test_images\")\n","images = [(path.name, open_image(path)) for path in folder.glob(\"*\")]\n","ids, inputs = map(list, zip(*images))\n","\n","input_data = tf.data.experimental.from_list(inputs).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:47.480438Z","iopub.status.busy":"2024-10-28T04:54:47.480061Z","iopub.status.idle":"2024-10-28T04:54:56.636980Z","shell.execute_reply":"2024-10-28T04:54:56.636013Z","shell.execute_reply.started":"2024-10-28T04:54:47.480403Z"},"trusted":true},"outputs":[],"source":["outputs = model.predict(input_data)[:, :-1]"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:54:56.639772Z","iopub.status.busy":"2024-10-28T04:54:56.639175Z","iopub.status.idle":"2024-10-28T04:54:56.644549Z","shell.execute_reply":"2024-10-28T04:54:56.643618Z","shell.execute_reply.started":"2024-10-28T04:54:56.639708Z"},"trusted":true},"outputs":[],"source":["cropnet_outputs = {id: output for id, output in zip(ids, outputs)}"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:55:47.475369Z","iopub.status.busy":"2024-10-28T04:55:47.474656Z","iopub.status.idle":"2024-10-28T04:55:48.280294Z","shell.execute_reply":"2024-10-28T04:55:48.279493Z","shell.execute_reply.started":"2024-10-28T04:55:47.475330Z"},"trusted":true},"outputs":[],"source":["device = cuda.get_current_device()\n","device.reset()"]},{"cell_type":"markdown","metadata":{},"source":["# Merging"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:55:02.485936Z","iopub.status.busy":"2024-10-28T04:55:02.485525Z","iopub.status.idle":"2024-10-28T04:55:02.492554Z","shell.execute_reply":"2024-10-28T04:55:02.491654Z","shell.execute_reply.started":"2024-10-28T04:55:02.485897Z"},"trusted":true},"outputs":[],"source":["final_answers = []\n","for image_id in vit_outputs.keys():\n","    vit_output = vit_outputs[image_id]\n","    convnext_output = convnext_outputs[image_id]\n","    cropnet_output = cropnet_outputs[image_id]\n","    final_output = (vit_output + convnext_output + cropnet_output) / 3\n","    final_answers.append({\"image_id\": image_id, \"label\": np.argmax(final_output)})"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:55:04.108452Z","iopub.status.busy":"2024-10-28T04:55:04.108061Z","iopub.status.idle":"2024-10-28T04:55:04.116370Z","shell.execute_reply":"2024-10-28T04:55:04.115178Z","shell.execute_reply.started":"2024-10-28T04:55:04.108414Z"},"trusted":true},"outputs":[],"source":["df = pd.DataFrame(final_answers)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:55:06.897132Z","iopub.status.busy":"2024-10-28T04:55:06.896132Z","iopub.status.idle":"2024-10-28T04:55:06.914949Z","shell.execute_reply":"2024-10-28T04:55:06.913827Z","shell.execute_reply.started":"2024-10-28T04:55:06.897079Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-10-28T04:55:08.197293Z","iopub.status.busy":"2024-10-28T04:55:08.196422Z","iopub.status.idle":"2024-10-28T04:55:08.205986Z","shell.execute_reply":"2024-10-28T04:55:08.205019Z","shell.execute_reply.started":"2024-10-28T04:55:08.197240Z"},"trusted":true},"outputs":[],"source":["df.to_csv(\"submission.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":1718836,"sourceId":13836,"sourceType":"competition"},{"datasetId":5938625,"sourceId":9709258,"sourceType":"datasetVersion"},{"datasetId":5940211,"sourceId":9711322,"sourceType":"datasetVersion"},{"datasetId":5959843,"sourceId":9737461,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
